p8105_hw2_rs4338
================
Rebecca Shyu
2024-09-28

``` r
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
```

## Problem 0:

- Create a public GitHub repo + local R Project: p8105_hw2_rs4338
- Create a single .Rmd file named p8105_hw2_rs4338.Rmd that renders to
  github_document
- Create a subdirectory (data) to store the local data files, and use
  relative paths to access these data files
- Submit a link to your repo via Courseworks
  - <https://github.com/rysgpd/p8105_hw2_rs4338>

## Problem 1:

- Reading & Cleaning the Dataset
  - Before processing, there were 32 variables and 1,868 rows included
    in the dataset. It looks like each row is an entrance and/or exit to
    a station, so there can be many rows for a single station. It
    provides information about the location (longitude/latitude of both
    entrance/exit and station) and characteristics (ex: ADA accessible,
    vending, staffing, etc) of the entrance/exit. There are also up to
    11 routes that may stop at the station.
  - Some data cleaning steps I took were: cleaning names using the
    `janitor` package, converted route8 to route11 to `character` when
    reading in the csv file, selecting the variables outlined in the
    homework assignment (line, station, name, station latitude /
    longitude, routes served, entrance type, entry, vending, and ADA
    compliance) through the `select` function, changed the entry and
    vending variables from YES/NO to 0/1 from the instructions through
    the `mutate` function.
  - The dimensions of the resulting table are 19 variables/columns and
    1,868 rows/entrances/exits.
  - The data is tidy because the columns are variable names not values,
    each column has multiple variables, and there is only one table.

``` r
initial_nyc_transit_df= read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") 

nyc_transit_df = read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
                          col_types = cols(
                            Route8 = "c",
                            Route9 = "c",
                            Route10 = "c",
                            Route11 = "c",
                            )
                          ) %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(
    entry = ifelse(entry == "YES", 1, 0),
    vending = ifelse(vending == "YES", 1, 0)
  )

knitr::kable(head(nyc_transit_df))
```

| line     | station_name | station_latitude | station_longitude | route1 | route2 | route3 | route4 | route5 | route6 | route7 | route8 | route9 | route10 | route11 | entrance_type | entry | vending | ada   |
|:---------|:-------------|-----------------:|------------------:|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:-------|:--------|:--------|:--------------|------:|--------:|:------|
| 4 Avenue | 25th St      |         40.66040 |         -73.99809 | R      | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA      | NA      | Stair         |     1 |       1 | FALSE |
| 4 Avenue | 25th St      |         40.66040 |         -73.99809 | R      | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA      | NA      | Stair         |     1 |       1 | FALSE |
| 4 Avenue | 36th St      |         40.65514 |         -74.00355 | N      | R      | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA      | NA      | Stair         |     1 |       1 | FALSE |
| 4 Avenue | 36th St      |         40.65514 |         -74.00355 | N      | R      | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA      | NA      | Stair         |     1 |       1 | FALSE |
| 4 Avenue | 36th St      |         40.65514 |         -74.00355 | N      | R      | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA      | NA      | Stair         |     1 |       1 | FALSE |
| 4 Avenue | 45th St      |         40.64894 |         -74.01001 | R      | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA     | NA      | NA      | Stair         |     1 |       1 | FALSE |

- Answer the following questions:
  - There are 465 distinct stations (using the `distinct` function)
  - There are 84 distinct stations that are ADA-compliant.
  - The proportion of station entrances/exits w/o vending that allow
    entrance is 0.3770492
  - Reformatting the data using the `unite` function, there are

  How many distinct stations serve the A train? Of the stations that
  serve the A train, how many are ADA compliant?

``` r
nyc_transit_df %>% 
  distinct(line, station_name) %>% 
  count() #465

nyc_transit_df %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(ada == TRUE) %>% 
  count() #84

no_vending_num = 
  nyc_transit_df %>% 
  filter(
    vending == 0
    ) %>% 
  count()

no_vending_yes_entry =
  nyc_transit_df %>% 
  filter(
    vending == 0,
    entry == 1
  ) %>% 
  count()

no_vending_yes_entry/no_vending_num
# 0.3770492 
```

## Problem 2:

``` r
mr_trash_df = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range="A2:N653") %>% 
  janitor::clean_names() %>% 
  mutate(
    sports_balls = round(sports_balls, digits = 0),
    sports_balls = as.integer(sports_balls),
    year = as.double(year),
    wheel = "mr_trash"
  )

prof_trash_df = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range="A2:M120") %>% 
  janitor::clean_names() %>% 
  mutate(
    wheel = "prof_trash"
  )

gwynnda_df = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", range="A2:L265") %>% 
  janitor::clean_names()%>% 
  mutate(
    wheel = "gwynnda"
  )

trash_df = 
  bind_rows(mr_trash_df, prof_trash_df, gwynnda_df) %>% 
  relocate(wheel)
  
summary(trash_df)
##     wheel              dumpster        month                year     
##  Length:1032        Min.   :  1.0   Length:1032        Min.   :2014  
##  Class :character   1st Qu.: 86.0   Class :character   1st Qu.:2018  
##  Mode  :character   Median :199.0   Mode  :character   Median :2020  
##                     Mean   :245.9                      Mean   :2020  
##                     3rd Qu.:393.2                      3rd Qu.:2022  
##                     Max.   :651.0                      Max.   :2024  
##                                                                      
##       date                        weight_tons    volume_cubic_yards
##  Min.   :2014-05-16 00:00:00.0   Min.   :0.610   Min.   : 5.00     
##  1st Qu.:2018-04-16 00:00:00.0   1st Qu.:2.540   1st Qu.:15.00     
##  Median :2020-12-26 00:00:00.0   Median :3.080   Median :15.00     
##  Mean   :2020-05-11 15:06:58.5   Mean   :3.038   Mean   :15.08     
##  3rd Qu.:2022-11-04 18:00:00.0   3rd Qu.:3.553   3rd Qu.:15.00     
##  Max.   :2024-06-11 00:00:00.0   Max.   :5.620   Max.   :20.00     
##                                                                    
##  plastic_bottles  polystyrene    cigarette_butts  glass_bottles   
##  Min.   :   0    Min.   :    0   Min.   :     0   Min.   :  0.00  
##  1st Qu.: 980    1st Qu.:  230   1st Qu.:  2800   1st Qu.: 10.00  
##  Median :1900    Median :  640   Median :  4800   Median : 18.00  
##  Mean   :2201    Mean   : 1383   Mean   : 13296   Mean   : 20.92  
##  3rd Qu.:2900    3rd Qu.: 2045   3rd Qu.: 12000   3rd Qu.: 28.00  
##  Max.   :9830    Max.   :11528   Max.   :310000   Max.   :110.00  
##  NA's   :1       NA's   :1       NA's   :1        NA's   :264     
##   plastic_bags      wrappers      sports_balls   homes_powered  
##  Min.   :    0   Min.   :    0   Min.   : 0.00   Min.   : 0.00  
##  1st Qu.:  220   1st Qu.:  900   1st Qu.: 6.00   1st Qu.:39.00  
##  Median :  470   Median : 1440   Median :12.00   Median :49.83  
##  Mean   :  927   Mean   : 2246   Mean   :13.98   Mean   :46.54  
##  3rd Qu.: 1115   3rd Qu.: 2580   3rd Qu.:20.00   3rd Qu.:58.08  
##  Max.   :13450   Max.   :20100   Max.   :56.00   Max.   :93.67  
##  NA's   :1       NA's   :118     NA's   :381     NA's   :69

trash_df %>% 
  filter(
    wheel == "mr_trash"
  ) %>% 
  summarise(sum(weight_tons, na.rm=TRUE))
## # A tibble: 1 × 1
##   `sum(weight_tons, na.rm = TRUE)`
##                              <dbl>
## 1                            2091.

trash_df %>% 
  filter(
    wheel == "gwynnda",
    year == 2022,
    month == "June"
  ) %>% 
  summarise(sum(cigarette_butts, na.rm=TRUE))
## # A tibble: 1 × 1
##   `sum(cigarette_butts, na.rm = TRUE)`
##                                  <dbl>
## 1                                18120
```

- Read and clean the Mr. Trash Wheel datasets to produce a single tidy
  dataset (trash_df)
- The final tidy dataset is comprised of 1032 total observations with
  651 from Mr. Trash Wheel, 118 from Professor Trash Wheel, and 263 from
  Gwynnda. The data spanned from 2014 to 2024 and included specific
  dates that the trash was collected. The dataset also includes the
  weight (tons) that ranged from 0.61 to 5.62, volume (cubic yards) that
  ranged from 5 to 20, and the number of pollutant objects found. These
  included plastic bottles, polystyrene, cigarette butts, glass bottles,
  plastic bags, wrappers, and sports balls. Almost all of the objects
  were counted for each dumpster check, except there were 264 missing
  for glass bottles, 118 for wrappers, and 381 for sports balls. The
  object with the largest mean among all dumpsters was cigarette butts
  at 13,296. The dataset also calculated the estimated number of homes
  powered by the collected trash.
- The total weight of trash collected by Professor Trash Wheel was
  2091.18 tons.
- The total number of cigarette butts collected by Gwynnda in June 2022
  was 1.812^{4}.

## Problem 3:

``` r
gbbo_bakers = read_csv("data/gbb_datasets/bakers.csv", show_col_types = FALSE) %>% 
  janitor::clean_names() %>% 
  separate(baker_name, c("baker", "baker_last_name"), sep=" ")

gbbo_bakes = read_csv("data/gbb_datasets/bakes.csv",
                      na=c("N/A", "NA", "UNKNOWN","", "Unknown"), show_col_types = FALSE) %>% 
  janitor::clean_names() %>%
  mutate(
    baker = replace(baker, baker == '"Jo"', "Jo")
  )

gbbo_results = read_csv("data/gbb_datasets/results.csv",
                        skip = 2, show_col_types = FALSE) %>% 
  janitor::clean_names() %>%
  mutate(
    baker = replace(baker, baker == "Joanne", "Jo")
  )
```

``` r
#anti_join(gbbo_results, gbbo_bakers, by=c("baker", "series"))

gbbo_df = 
  right_join(gbbo_bakes, gbbo_results, by=c("baker", "series", "episode")) %>% 
  right_join(gbbo_bakers, by = c("baker", "series")) %>% 
  filter(
    !is.na(result)
  )

gbbo_df
```

    ## # A tibble: 710 × 11
    ##    series episode baker     signature_bake         show_stopper technical result
    ##     <dbl>   <dbl> <chr>     <chr>                  <chr>            <dbl> <chr> 
    ##  1      1       1 Annetha   "Light Jamaican Black… Red, White …         2 IN    
    ##  2      1       1 David     "Chocolate Orange Cak… Black Fores…         3 IN    
    ##  3      1       1 Edd       "Caramel Cinnamon and… <NA>                 1 IN    
    ##  4      1       1 Jasminder "Fresh Mango and Pass… <NA>                NA IN    
    ##  5      1       1 Jonathan  "Carrot Cake with Lim… Three Tiere…         9 IN    
    ##  6      1       1 Lea       "Cranberry and Pistac… Raspberries…        10 OUT   
    ##  7      1       1 Louise    "Carrot and Orange Ca… Never Fail …        NA IN    
    ##  8      1       1 Mark      "Sticky Marmalade Tea… Heart-shape…        NA OUT   
    ##  9      1       1 Miranda   "Triple Layered Brown… Three Tiere…         8 IN    
    ## 10      1       1 Ruth      "Three Tiered Lemon D… Classic Cho…        NA IN    
    ## # ℹ 700 more rows
    ## # ℹ 4 more variables: baker_last_name <chr>, baker_age <dbl>,
    ## #   baker_occupation <chr>, hometown <chr>

``` r
write_csv(gbbo_df, file="data/gbb_datasets/final_gbbo_df.csv")
```

- The data cleaning process:

  - I noticed that the gbbo_bakers dataset about the bakers had their
    full name, but all of the other files only had a first name, so I
    separated the full name into two fields (“baker” which was the their
    first name, and “baker_last_name”)
  - There were multiple versions of missing data: “N/A”, “Unknown”,
    “NA”, and “UNKNOWN” for gbbo_bakes.
  - There were two rows at the top of the csv for gbbo_results which
    affected the read_csv function, so I skipped those two rows.
  - When doing the `anti_join` function, I saw that Jo Wheatley of
    season 2 was referred as “Jo” and Josephine in gbbo_bakes and
    gbbo_results, so I replaced those with Jo so it would join easily.
  - I noticed that there was no data in gbbo_bakes past season 8
    (missing seasons 9 and 10), so I wonder what happened to the dishes
    on those seasons, but there were bakers and results for those
    seasons so I ended up including them anyways.
  - I was also wondering why there were some bakers whose technical
    placing was missing.
  - I joined the tables using the gbbo_results as the base table based
    on the first name and the season of the bakers. There were multiple
    bakers with the same name, but none on the same season.
  - I also dropped all rows that had NA in the result column because
    those weeks were after the bakers were eliminated and provided no
    additional information.

- The final dataset has 710 rows, with each row being how a baker did
  for a single episode, including the dishes they made, their placing in
  the technical challenge, and the ultimate results from that
  week/episode. The demographic information (age, job, hometown) were
  repeated for each episode the baker was on and were less important so
  they were all in the later columns of the final dataset.

- Here is the reader-friendly table of the winners and star bakers of
  each episode and season.

- Most of the winners were predictable, being star bakers for 3-4
  episodes before winning it all. There were, however, a few surprises

  1.  Richard of Season 5: he was star baker 5(!!) times, but did not
      win overall.
  2.  Steph of Season 10: they were star baker 4 times but also did not
      win overall.
  3.  Most impressively, was David of Season 10: he was only star baker
      once and won it overall.

``` r
gbbo_winners =
  gbbo_df %>% 
    filter(
      series >= 5,
      result == "STAR BAKER" | result == "WINNER"
    )

knitr::kable(head(gbbo_winners, 5))
```

| series | episode | baker   | signature_bake                  | show_stopper                  | technical | result     | baker_last_name | baker_age | baker_occupation         | hometown                         |
|-------:|--------:|:--------|:--------------------------------|:------------------------------|----------:|:-----------|:----------------|----------:|:-------------------------|:---------------------------------|
|      5 |       1 | Nancy   | Coffee and Hazelnut Swiss Roll  | Jaffa Orange Cakes            |         1 | STAR BAKER | Birtwhistle     |        60 | Retired Practice Manager | Barton-upon-Humber, Lincolnshire |
|      5 |       2 | Richard | Rosemary Seeded Crackers        | Pirates!                      |         1 | STAR BAKER | Burr            |        38 | Builder                  | Mill Hill, London                |
|      5 |       3 | Luis    | Opposites Attract Rolls         | Roscón de Reyes               |         2 | STAR BAKER | Troyano         |        42 | Graphic Designer         | Poynton, Cheshire                |
|      5 |       4 | Richard | Black Forest Chocolate Fondants | Tiramisu Baked Alaska         |         5 | STAR BAKER | Burr            |        38 | Builder                  | Mill Hill, London                |
|      5 |       5 | Kate    | Rhubarb and Custard Tart        | Rhubarb, Prune and Apple Pies |         3 | STAR BAKER | Henry           |        41 | Furniture Restorer       | Brighton, East Sussex            |

``` r
gbbo_winners %>% 
  count(series, baker) %>% 
  arrange(desc(n)) %>% 
  mutate(
    winner = ifelse(baker %in% pull(gbbo_winners %>% filter (result == "WINNER") %>% select(baker)), 1, 0)
  )
```

    ## # A tibble: 33 × 4
    ##    series baker       n winner
    ##     <dbl> <chr>   <int>  <dbl>
    ##  1      5 Richard     5      0
    ##  2      6 Nadiya      4      1
    ##  3      7 Candice     4      1
    ##  4     10 Steph       4      0
    ##  5      6 Ian         3      0
    ##  6      8 Sophie      3      1
    ##  7      8 Steven      3      0
    ##  8      9 Rahul       3      1
    ##  9      5 Nancy       2      1
    ## 10      7 Andrew      2      0
    ## # ℹ 23 more rows

``` r
gbbo_viewers = read_csv("data/gbb_datasets/viewers.csv", show_col_types = FALSE) %>% 
  janitor::clean_names() 

cleaned_gbbo_viewers =
  gbbo_viewers %>% 
  pivot_longer(
    series_1:series_10,
    names_to = "series",
    values_to = "viewers",
    names_prefix = "series_"
  ) %>% 
  relocate(series) %>% 
  mutate(
    series = as.numeric(series)
  ) %>% 
  arrange(series, episode)

knitr::kable(head(cleaned_gbbo_viewers, 10))
```

| series | episode | viewers |
|-------:|--------:|--------:|
|      1 |       1 |    2.24 |
|      1 |       2 |    3.00 |
|      1 |       3 |    3.00 |
|      1 |       4 |    2.60 |
|      1 |       5 |    3.03 |
|      1 |       6 |    2.75 |
|      1 |       7 |      NA |
|      1 |       8 |      NA |
|      1 |       9 |      NA |
|      1 |      10 |      NA |

- The dataset is missing a few entries in season 1 (episodes 7-10) and
  season 2 (episodes 9 and 10).
- The average viewership in season 1 was 2.77
- The average viewership in season 5 was 10.0393
