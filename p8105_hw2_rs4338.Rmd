---
title: "p8105_hw2_rs4338"
author: "Rebecca Shyu"
date: "2024-09-28"
output: github_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
```

## Problem 0:

* Create a public GitHub repo + local R Project: p8105_hw2_rs4338
* Create a single .Rmd file named p8105_hw2_rs4338.Rmd that renders to github_document
* Create a subdirectory (data) to store the local data files, and use relative paths to access these data files
* Submit a link to your repo via Courseworks
  - https://github.com/rysgpd/p8105_hw2_rs4338

## Problem 1:

* Reading & Cleaning the Dataset
  - Before processing, there were 32 variables and 1,868 rows included in the dataset. It looks like each row is an entrance and/or exit to a station, so there can be many rows for a single station. It provides information about the location (longitude/latitude of both entrance/exit and station) and characteristics (ex: ADA accessible, vending, staffing, etc) of the entrance/exit. There are also up to 11 routes that may stop at the station. 
  - Some data cleaning steps I took were: cleaning names using the `janitor` package,  converted route8 to route11 to `character` when reading in the csv file, selecting the variables outlined in the homework assignment (line, station, name, station latitude / longitude, routes served, entrance type, entry, vending, and ADA compliance) through the `select` function, changed the entry and vending variables from YES/NO to 0/1 from the instructions through the `mutate` function.
  - The dimensions of the resulting table are 19 variables/columns and 1,868 rows/entrances/exits.
  - The data is tidy because the columns are variable names not values, each column has multiple variables, and there is only one table.

```{r prob1_readcsv, message=FALSE}
initial_nyc_transit_df= read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") 

nyc_transit_df = read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
                          col_types = cols(
                            Route8 = "c",
                            Route9 = "c",
                            Route10 = "c",
                            Route11 = "c",
                            )
                          ) %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(
    entry = ifelse(entry == "YES", 1, 0),
    vending = ifelse(vending == "YES", 1, 0)
  )

nyc_transit_df
```

* Answer the following questions:
  - There are `r nyc_transit_df %>% distinct(line, station_name) %>% count()` distinct stations (using the `distinct` function)
  - There are `r nyc_transit_df %>% distinct(line, station_name, .keep_all = TRUE) %>% filter(ada == TRUE) %>% count()` distinct stations that are ADA-compliant. 
  - The proportion of station entrances/exits w/o vending that allow entrance is `r (nyc_transit_df %>% filter(vending == 0, entry == 1) %>% count()) / (nyc_transit_df %>% filter(vending == 0) %>% count())`
  - Reformatting the data using the `unite` function, there are 
  
  How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?


```{r probl1_questions, eval = FALSE}

nyc_transit_df %>% 
  distinct(line, station_name) %>% 
  count() #465

nyc_transit_df %>% 
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(ada == TRUE) %>% 
  count() #84

no_vending_num = 
  nyc_transit_df %>% 
  filter(
    vending == 0
    ) %>% 
  count()

no_vending_yes_entry =
  nyc_transit_df %>% 
  filter(
    vending == 0,
    entry == 1
  ) %>% 
  count()

no_vending_yes_entry/no_vending_num
# 0.3770492	
    
```

## Problem 2:


```{r trash_prob2, collapse = TRUE, message=FALSE}
mr_trash_df = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range="A2:N653") %>% 
  janitor::clean_names() %>% 
  mutate(
    sports_balls = round(sports_balls, digits = 0),
    sports_balls = as.integer(sports_balls),
    year = as.double(year),
    wheel = "mr_trash"
  )

prof_trash_df = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range="A2:M120") %>% 
  janitor::clean_names() %>% 
  mutate(
    wheel = "prof_trash"
  )

gwynnda_df = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", range="A2:L265") %>% 
  janitor::clean_names()%>% 
  mutate(
    wheel = "gwynnda"
  )

trash_df = 
  bind_rows(mr_trash_df, prof_trash_df, gwynnda_df) %>% 
  relocate(wheel)
  
summary(trash_df)

trash_df %>% 
  filter(
    wheel == "mr_trash"
  ) %>% 
  summarise(sum(weight_tons, na.rm=TRUE))

trash_df %>% 
  filter(
    wheel == "gwynnda",
    year == 2022,
    month == "June"
  ) %>% 
  summarise(sum(cigarette_butts, na.rm=TRUE))
```

* Read and clean the Mr. Trash Wheel datasets to produce a single tidy dataset (trash_df)
* The final tidy dataset is comprised of 1032 total observations with 651 from Mr. Trash Wheel, 118 from Professor Trash Wheel, and 263 from Gwynnda. The data spanned from 2014 to 2024 and included specific dates that the trash was collected. The dataset also includes the weight (tons) that ranged from 0.61 to 5.62, volume (cubic yards) that ranged from 5 to 20, and the number of pollutant objects found. These included plastic bottles, polystyrene, cigarette butts, glass bottles, plastic bags, wrappers, and sports balls. Almost all of the objects were counted for each dumpster check, except there were 264 missing for glass bottles, 118 for wrappers, and 381 for sports balls. The object with the largest mean among all dumpsters was cigarette butts at 13,296. The dataset also calculated the estimated number of homes powered by the collected trash.
* The total weight of trash collected by Professor Trash Wheel was `r trash_df %>% filter(wheel == "mr_trash") %>% summarise(sum(weight_tons, na.rm=TRUE))` tons.
* The total number of cigarette butts collected by Gwynnda in June 2022 was `r trash_df %>% filter(wheel == "gwynnda",year == 2022,month == "June") %>% summarise(sum(cigarette_butts, na.rm=TRUE))`.

## Problem 3:

```{r prob3}

gbbo_bakers = read_csv("data/gbb_datasets/bakers.csv", show_col_types = FALSE) %>% 
  janitor::clean_names() %>% 
  separate(baker_name, c("baker", "baker_last_name"), sep=" ")

gbbo_bakes = read_csv("data/gbb_datasets/bakes.csv",
                      na=c("N/A", "NA", "UNKNOWN","", "Unknown"), show_col_types = FALSE) %>% 
  janitor::clean_names() %>%
  mutate(
    baker = replace(baker, baker == '"Jo"', "Jo")
  )

gbbo_results = read_csv("data/gbb_datasets/results.csv",
                        skip = 2, show_col_types = FALSE) %>% 
  janitor::clean_names() %>%
  mutate(
    baker = replace(baker, baker == "Joanne", "Jo")
  )

gbbo_viewers = read_csv("data/gbb_datasets/viewers.csv", show_col_types = FALSE) %>% 
  janitor::clean_names()

```

```{r}
#anti_join(gbbo_results, gbbo_bakers, by=c("baker", "series"))

gbbo_df = 
  right_join(gbbo_bakes, gbbo_results, by=c("baker", "series", "episode")) %>% 
  right_join(gbbo_bakers, by = c("baker", "series")) %>% 
  filter(
    !is.na(result)
  )

gbbo_df

write_csv(gbbo_df, file="data/gbb_datasets/final_gbbo_df.csv")
```



* The data cleaning process:
  - I noticed that the gbbo_bakers dataset about the bakers had their full name, but all of the other files only had a first name, so I separated the full name into two fields ("baker" which was the their first name, and "baker_last_name")
  - There were multiple versions of missing data: "N/A", "Unknown", "NA", and "UNKNOWN" for gbbo_bakes.
  - There were two rows at the top of the csv for gbbo_results which affected the read_csv function, so I skipped those two rows.
  - When doing the `anti_join` function, I saw that Jo Wheatley of season 2 was referred as "Jo" and Josephine in gbbo_bakes and gbbo_results, so I replaced those with Jo so it would join easily.
  - I noticed that there was no data in gbbo_bakes past season 8 (missing seasons 9 and 10), so I wonder what happened to the dishes on those seasons, but there were bakers and results for those seasons so I ended up including them anyways. 
  - I was also wondering why there were some bakers whose technical placing was missing.
  - I joined the tables using the gbbo_results as the base table based on the first name and the season of the bakers. There were multiple bakers with the same name, but none on the same season.
  - I also dropped all rows that had NA in the result column because those weeks were after the bakers were eliminated and provided no additional information. 
* The final dataset has `r gbbo_df %>% count()` rows, with each row being how a baker did for a single episode, including the dishes they made, their placing in the technical challenge, and the ultimate results from that week/episode. The demographic information (age, job, hometown) were repeated for each episode the baker was on and were less important so they were all in the later columns of the final dataset.

* Here is the reader-friendly table of the winners and star bakers of each episode and season. 
* Most of the winners were predictable, being star bakers for 3-4 episodes before winning it all. There were, however, a few surprises
  1. Richard of Season 5: he was star baker 5(!!) times, but did not win overall. 
  2. Steph of Season 10: they were star baker 4 times but also did not win overall.
  3. Most impressively, was David of Season 10: he was only star baker once and won it overall. 

```{r}
gbbo_winners =
  gbbo_df %>% 
    filter(
      series >= 5,
      result == "STAR BAKER" | result == "WINNER"
    )

gbbo_winners

gbbo_winners %>% 
  count(series, baker) %>% 
  arrange(desc(n)) %>% 
  mutate(
    winner = ifelse(baker %in% pull(gbbo_winners %>% filter (result == "WINNER") %>% select(baker)), 1, 0)
  )
```

